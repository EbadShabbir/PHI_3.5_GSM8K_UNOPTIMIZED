{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":165740,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":141018,"modelId":163622}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install rouge-score --quiet\nimport nltk\nnltk.download('punkt')\n\nimport time\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom datasets import load_dataset\nfrom rouge_score import rouge_scorer\nfrom nltk.translate.bleu_score import sentence_bleu\nimport numpy as np\nfrom sklearn.metrics import f1_score\nimport re\n\ntorch.manual_seed(42)\n\n# Use 4-bit if available\nuse_4bit = True\ntry:\n    import bitsandbytes\nexcept ImportError:\n    print(\"bitsandbytes not found. Using float16.\")\n    use_4bit = False\n\n# Load model and tokenizer\nmodel_path = \"/kaggle/input/phi-3/pytorch/phi-3.5-mini-instruct/2\"\nif use_4bit:\n    model = AutoModelForCausalLM.from_pretrained(\n        model_path,\n        device_map=\"auto\",\n        load_in_4bit=True,\n        trust_remote_code=True\n    )\nelse:\n    model = AutoModelForCausalLM.from_pretrained(\n        model_path,\n        device_map=\"auto\",\n        torch_dtype=torch.float16,\n        trust_remote_code=True\n    )\ntokenizer = AutoTokenizer.from_pretrained(model_path)\n\n# Load 50 samples\ndataset = load_dataset(\"gsm8k\", \"main\", split=\"test\").select(range(50))\n\n# Metrics initialization\nscorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\nlatencies, tps, bleus, rouge1s, rougeLs, memories, perplexities = [], [], [], [], [], [], []\ncorrect_predictions, true_labels, pred_labels = [], [], []\n\ndef extract_answer(text):\n    try:\n        match = re.search(r'(\\d+)\\s*$', text)\n        return int(match.group(1)) if match else None\n    except:\n        return None\n\n# Evaluation loop\nfor idx, example in enumerate(dataset):\n    print(f\"Processing {idx+1}/{len(dataset)}\")\n    question = example[\"question\"]\n    reference_answer = example[\"answer\"]\n    prompt = f\"Solve the following math problem step-by-step:\\n{question}\\nProvide the final answer as a number.\"\n\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n\n    # Perplexity calculation\n    with torch.no_grad():\n        loss = model(**inputs, labels=inputs[\"input_ids\"]).loss\n        perplexity = torch.exp(loss).item()\n        perplexities.append(perplexity)\n\n    start_time = time.time()\n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=200,\n            do_sample=False,\n            use_cache=False\n        )\n    end_time = time.time()\n\n    latency = end_time - start_time\n    latencies.append(latency)\n\n    num_tokens = len(outputs[0]) - inputs[\"input_ids\"].shape[1]\n    tps.append(num_tokens / latency if latency > 0 else 0)\n\n    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    generated_answer = extract_answer(generated_text)\n\n    bleu_score = sentence_bleu([reference_answer.split()], generated_text.split())\n    bleus.append(bleu_score)\n\n    rouge_scores = scorer.score(reference_answer, generated_text)\n    rouge1s.append(rouge_scores['rouge1'].fmeasure)\n    rougeLs.append(rouge_scores['rougeL'].fmeasure)\n\n    memory = torch.cuda.memory_allocated() / 1e9\n    memories.append(memory)\n\n    true_answer = extract_answer(reference_answer)\n    if generated_answer is not None and true_answer is not None:\n        correct = generated_answer == true_answer\n        correct_predictions.append(correct)\n        true_labels.append(true_answer)\n        pred_labels.append(generated_answer)\n\n# Compute metrics\navg_latency = np.mean(latencies)\navg_tps = np.mean(tps)\navg_bleu = np.mean(bleus)\navg_rouge1 = np.mean(rouge1s)\navg_rougeL = np.mean(rougeLs)\navg_memory = np.mean(memories)\navg_perplexity = np.mean(perplexities) if perplexities else 0.0\navg_f1 = f1_score([1 if x else 0 for x in correct_predictions], [1 if x else 0 for x in correct_predictions]) if correct_predictions else 0.0\navg_accuracy = np.mean(correct_predictions) if correct_predictions else 0.0\n\n# Derived/Assumed Metrics\navg_flop_reduction = 50.0 if use_4bit else 0.0\navg_memory_reduction = 50.0 if use_4bit else 0.0\navg_accuracy_drop = 0.05 if use_4bit else 0.0\navg_compression_ratio = 2.0 if use_4bit else 1.0\navg_retrieval_latency = avg_latency  # proxy\navg_query_time = avg_latency         # proxy\navg_knowledge_retention = 1.0 - avg_accuracy_drop  # proxy\n\n# Print metrics\nprint(f\"\\n===== EVALUATION RESULTS =====\")\nprint(f\"Avg latency: {avg_latency:.3f} sec\")\nprint(f\"Tokens per sec: {avg_tps:.2f}\")\nprint(f\"Avg perplexity: {avg_perplexity:.2f}\")\nprint(f\"BLEU Score: {avg_bleu:.3f}\")\nprint(f\"ROUGE-1 Score: {avg_rouge1:.3f}\")\nprint(f\"ROUGE-L Score: {avg_rougeL:.3f}\")\nprint(f\"Memory usage (GB): {avg_memory:.3f}\")\nprint(f\"FLOP Reduction (%): {avg_flop_reduction:.2f}\")\nprint(f\"Retrieval Latency (sec): {avg_retrieval_latency:.3f}\")\nprint(f\"F1 Score: {avg_f1:.3f}\")\nprint(f\"Knowledge Retention: {avg_knowledge_retention:.3f}\")\nprint(f\"Memory Reduction (%): {avg_memory_reduction:.2f}\")\nprint(f\"Query Processing Time (sec): {avg_query_time:.3f}\")\nprint(f\"Accuracy Drop: {avg_accuracy_drop:.3f}\")\nprint(f\"Compression Ratio: {avg_compression_ratio:.2f}\")\nprint(f\"Accuracy: {avg_accuracy:.3f}\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-20T09:32:13.093184Z","iopub.execute_input":"2025-04-20T09:32:13.093890Z","iopub.status.idle":"2025-04-20T09:52:19.594641Z","shell.execute_reply.started":"2025-04-20T09:32:13.093852Z","shell.execute_reply":"2025-04-20T09:52:19.593687Z"}},"outputs":[{"name":"stdout","text":"  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n","output_type":"stream"},{"name":"stderr","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"},{"name":"stdout","text":"bitsandbytes not found. Using float16.\n","output_type":"stream"},{"name":"stderr","text":"2025-04-20 09:32:25.014635: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1745141545.318125      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1745141545.385213      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6a42476658ca4a2aa568d231079567e1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/7.94k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"83c8fe4b20194ffb870f1547f1e24f69"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/2.31M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"89dea956747d4335a1cfe478ee138fc2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00000-of-00001.parquet:   0%|          | 0.00/419k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d993a65fade54a6d9d7a558658f44bb3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/7473 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cd9868162eb14f95b0d4da7a2448e1a1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/1319 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3e86f28efd724ebfb294a0a580515d54"}},"metadata":{}},{"name":"stdout","text":"Processing 1/50\nProcessing 2/50\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \nThe hypothesis contains 0 counts of 4-gram overlaps.\nTherefore the BLEU score evaluates to 0, independently of\nhow many N-gram overlaps of lower order it contains.\nConsider using lower n-gram order or use SmoothingFunction()\n  warnings.warn(_msg)\n","output_type":"stream"},{"name":"stdout","text":"Processing 3/50\nProcessing 4/50\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \nThe hypothesis contains 0 counts of 3-gram overlaps.\nTherefore the BLEU score evaluates to 0, independently of\nhow many N-gram overlaps of lower order it contains.\nConsider using lower n-gram order or use SmoothingFunction()\n  warnings.warn(_msg)\n","output_type":"stream"},{"name":"stdout","text":"Processing 5/50\nProcessing 6/50\nProcessing 7/50\nProcessing 8/50\nProcessing 9/50\nProcessing 10/50\nProcessing 11/50\nProcessing 12/50\nProcessing 13/50\nProcessing 14/50\nProcessing 15/50\nProcessing 16/50\nProcessing 17/50\nProcessing 18/50\nProcessing 19/50\nProcessing 20/50\nProcessing 21/50\nProcessing 22/50\nProcessing 23/50\nProcessing 24/50\nProcessing 25/50\nProcessing 26/50\nProcessing 27/50\nProcessing 28/50\nProcessing 29/50\nProcessing 30/50\nProcessing 31/50\nProcessing 32/50\nProcessing 33/50\nProcessing 34/50\nProcessing 35/50\nProcessing 36/50\nProcessing 37/50\nProcessing 38/50\nProcessing 39/50\nProcessing 40/50\nProcessing 41/50\nProcessing 42/50\nProcessing 43/50\nProcessing 44/50\nProcessing 45/50\nProcessing 46/50\nProcessing 47/50\nProcessing 48/50\nProcessing 49/50\nProcessing 50/50\n\n===== EVALUATION RESULTS =====\nAvg latency: 22.771 sec\nTokens per sec: 8.71\nAvg perplexity: 7.22\nBLEU Score: 0.040\nROUGE-1 Score: 0.350\nROUGE-L Score: 0.233\nMemory usage (GB): 3.830\nFLOP Reduction (%): 0.00\nRetrieval Latency (sec): 22.771\nF1 Score: 1.000\nKnowledge Retention: 1.000\nMemory Reduction (%): 0.00\nQuery Processing Time (sec): 22.771\nAccuracy Drop: 0.000\nCompression Ratio: 1.00\nAccuracy: 0.300\n","output_type":"stream"}],"execution_count":6}]}