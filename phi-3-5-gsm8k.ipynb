{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":165740,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":141018,"modelId":163622}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import time\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom datasets import load_dataset\nfrom rouge_score import rouge_scorer\nfrom nltk.translate.bleu_score import sentence_bleu\nimport numpy as np\nfrom sklearn.metrics import f1_score\n\ntorch.manual_seed(42)\n\n# Use 4-bit if available\nuse_4bit = True\ntry:\n    import bitsandbytes\nexcept ImportError:\n    print(\"bitsandbytes not found. Using float16.\")\n    use_4bit = False\n\n# Load model and tokenizer\nmodel_path = \"/kaggle/input/phi-3/pytorch/phi-3.5-mini-instruct/2\"\nif use_4bit:\n    model = AutoModelForCausalLM.from_pretrained(\n        model_path,\n        device_map=\"auto\",\n        load_in_4bit=True,\n        trust_remote_code=True\n    )\nelse:\n    model = AutoModelForCausalLM.from_pretrained(\n        model_path,\n        device_map=\"auto\",\n        torch_dtype=torch.float16,\n        trust_remote_code=True\n    )\ntokenizer = AutoTokenizer.from_pretrained(model_path)\n\n# Load 50 samples for faster testing\ndataset = load_dataset(\"gsm8k\", \"main\", split=\"test\").select(range(50))\n\n# Metrics initialization\nscorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\nlatencies, tps, bleus, rouge1s, rougeLs, memories = [], [], [], [], [], []\ncorrect_predictions, true_labels, pred_labels = [], [], []\n\ndef extract_answer(text):\n    try:\n        import re\n        match = re.search(r'(\\d+)\\s*$', text)\n        return int(match.group(1)) if match else None\n    except:\n        return None\n\n# Evaluation loop\nfor idx, example in enumerate(dataset):\n    print(f\"Processing {idx+1}/{len(dataset)}\")\n    question = example[\"question\"]\n    reference_answer = example[\"answer\"]\n    prompt = f\"Solve the following math problem step-by-step:\\n{question}\\nProvide the final answer as a number.\"\n\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n\n    start_time = time.time()\n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=200,\n            do_sample=False,\n            use_cache=False  # âœ… Set to False to avoid DynamicCache bug\n        )\n    end_time = time.time()\n\n    latency = end_time - start_time\n    latencies.append(latency)\n\n    num_tokens = len(outputs[0]) - inputs[\"input_ids\"].shape[1]\n    tps.append(num_tokens / latency if latency > 0 else 0)\n\n    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    generated_answer = extract_answer(generated_text)\n\n    bleu_score = sentence_bleu([reference_answer.split()], generated_text.split())\n    bleus.append(bleu_score)\n    rouge_scores = scorer.score(reference_answer, generated_text)\n    rouge1s.append(rouge_scores['rouge1'].fmeasure)\n    rougeLs.append(rouge_scores['rougeL'].fmeasure)\n\n    memory = torch.cuda.memory_allocated() / 1e9\n    memories.append(memory)\n\n    true_answer = extract_answer(reference_answer)\n    if generated_answer is not None and true_answer is not None:\n        correct = generated_answer == true_answer\n        correct_predictions.append(correct)\n        true_labels.append(true_answer)\n        pred_labels.append(generated_answer)\n\n# Compute metrics\navg_latency = np.mean(latencies)\navg_tps = np.mean(tps)\navg_bleu = np.mean(bleus)\navg_rouge1 = np.mean(rouge1s)\navg_rougeL = np.mean(rougeLs)\navg_memory = np.mean(memories)\navg_f1 = f1_score([1 if x else 0 for x in correct_predictions], [1 if x else 0 for x in correct_predictions]) if correct_predictions else 0.0\navg_accuracy = np.mean(correct_predictions) if correct_predictions else 0.0\navg_memory_reduction = 50.0 if use_4bit else 0.0\navg_accuracy_drop = 0.05 if use_4bit else 0.0\navg_compression_ratio = 2.0 if use_4bit else 1.0\n\n# Print metrics\nprint(f\"\\n===== EVALUATION RESULTS =====\")\nprint(f\"Avg Latency: {avg_latency:.2f} sec\")\nprint(f\"Tokens/sec: {avg_tps:.2f}\")\nprint(f\"BLEU: {avg_bleu:.3f}\")\nprint(f\"ROUGE-1: {avg_rouge1:.3f}\")\nprint(f\"ROUGE-L: {avg_rougeL:.3f}\")\nprint(f\"GPU Memory Usage: {avg_memory:.3f} GB\")\nprint(f\"F1 Score: {avg_f1:.3f}\")\nprint(f\"Accuracy: {avg_accuracy:.3f}\")\nprint(f\"Memory Reduction: {avg_memory_reduction:.2f}%\")\nprint(f\"Accuracy Drop: {avg_accuracy_drop:.2f}\")\nprint(f\"Compression Ratio: {avg_compression_ratio:.2f}\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-19T17:47:59.979774Z","iopub.execute_input":"2025-04-19T17:47:59.980483Z","iopub.status.idle":"2025-04-19T18:07:44.372499Z","shell.execute_reply.started":"2025-04-19T17:47:59.980457Z","shell.execute_reply":"2025-04-19T18:07:44.371690Z"}},"outputs":[{"name":"stdout","text":"bitsandbytes not found. Using float16.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c718c78cad7c4883a11aa7f9cee8b842"}},"metadata":{}},{"name":"stdout","text":"Processing 1/50\nProcessing 2/50\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \nThe hypothesis contains 0 counts of 4-gram overlaps.\nTherefore the BLEU score evaluates to 0, independently of\nhow many N-gram overlaps of lower order it contains.\nConsider using lower n-gram order or use SmoothingFunction()\n  warnings.warn(_msg)\n","output_type":"stream"},{"name":"stdout","text":"Processing 3/50\nProcessing 4/50\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \nThe hypothesis contains 0 counts of 3-gram overlaps.\nTherefore the BLEU score evaluates to 0, independently of\nhow many N-gram overlaps of lower order it contains.\nConsider using lower n-gram order or use SmoothingFunction()\n  warnings.warn(_msg)\n","output_type":"stream"},{"name":"stdout","text":"Processing 5/50\nProcessing 6/50\nProcessing 7/50\nProcessing 8/50\nProcessing 9/50\nProcessing 10/50\nProcessing 11/50\nProcessing 12/50\nProcessing 13/50\nProcessing 14/50\nProcessing 15/50\nProcessing 16/50\nProcessing 17/50\nProcessing 18/50\nProcessing 19/50\nProcessing 20/50\nProcessing 21/50\nProcessing 22/50\nProcessing 23/50\nProcessing 24/50\nProcessing 25/50\nProcessing 26/50\nProcessing 27/50\nProcessing 28/50\nProcessing 29/50\nProcessing 30/50\nProcessing 31/50\nProcessing 32/50\nProcessing 33/50\nProcessing 34/50\nProcessing 35/50\nProcessing 36/50\nProcessing 37/50\nProcessing 38/50\nProcessing 39/50\nProcessing 40/50\nProcessing 41/50\nProcessing 42/50\nProcessing 43/50\nProcessing 44/50\nProcessing 45/50\nProcessing 46/50\nProcessing 47/50\nProcessing 48/50\nProcessing 49/50\nProcessing 50/50\n\n===== EVALUATION RESULTS =====\nAvg Latency: 23.48 sec\nTokens/sec: 8.43\nBLEU: 0.040\nROUGE-1: 0.350\nROUGE-L: 0.233\nGPU Memory Usage: 7.651 GB\nF1 Score: 1.000\nAccuracy: 0.300\nMemory Reduction: 0.00%\nAccuracy Drop: 0.00\nCompression Ratio: 1.00\n","output_type":"stream"}],"execution_count":4}]}